{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation Notebook\n",
        "## Image Colorization using GAN\n",
        "\n",
        "This notebook provides comprehensive evaluation of the trained colorization model.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "1. PSNR (Peak Signal-to-Noise Ratio)\n",
        "2. SSIM (Structural Similarity Index)\n",
        "3. Colorfulness Metric\n",
        "4. L1/L2 Error\n",
        "\n",
        "**Contents:**\n",
        "1. Load Trained Model\n",
        "2. Quantitative Evaluation\n",
        "3. Qualitative Evaluation\n",
        "4. Error Analysis\n",
        "5. Comparison Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "# Import modules\n",
        "from src.models import UNetGenerator\n",
        "from src.preprocessing import create_dataloaders, lab2rgb\n",
        "from src.evaluation import Evaluator, calculate_psnr, calculate_ssim, calculate_colorfulness\n",
        "from src.utils import denormalize_lab, create_comparison_grid\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Device selection\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model\n",
        "generator = UNetGenerator(in_channels=1, out_channels=2, features=64).to(device)\n",
        "\n",
        "# Load trained weights\n",
        "model_path = '../trained_models/generator_final.pth'\n",
        "if os.path.exists(model_path):\n",
        "    generator.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(f\"Loaded model from: {model_path}\")\n",
        "else:\n",
        "    # Try loading from checkpoints\n",
        "    checkpoint_path = '../results/checkpoints/best_model.pth'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        print(f\"Loaded model from checkpoint: {checkpoint_path}\")\n",
        "    else:\n",
        "        print(\"WARNING: No trained model found!\")\n",
        "\n",
        "generator.eval()\n",
        "print(\"Model ready for evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "train_loader, val_loader, test_loader = create_dataloaders(\n",
        "    data_dir='../data/train',\n",
        "    batch_size=16,\n",
        "    image_size=256,\n",
        "    val_split=0.1,\n",
        "    test_split=0.1,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"Test set: {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = Evaluator(\n",
        "    generator=generator,\n",
        "    device=device,\n",
        "    save_dir='../results/evaluation'\n",
        ")\n",
        "\n",
        "# Run evaluation on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "results = evaluator.evaluate_dataset(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(\"=\"*60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\")\n",
        "print(\"1. PSNR (Peak Signal-to-Noise Ratio)\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Mean: {results['psnr']['mean']:.2f} dB\")\n",
        "print(f\"   Std:  {results['psnr']['std']:.2f} dB\")\n",
        "print(f\"   Min:  {results['psnr']['min']:.2f} dB\")\n",
        "print(f\"   Max:  {results['psnr']['max']:.2f} dB\")\n",
        "print(\"\")\n",
        "print(\"2. SSIM (Structural Similarity)\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Mean: {results['ssim']['mean']:.4f}\")\n",
        "print(f\"   Std:  {results['ssim']['std']:.4f}\")\n",
        "print(\"\")\n",
        "print(\"3. Pixel Error\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   L1 Error: {results['l1_error']['mean']:.4f}\")\n",
        "print(f\"   L2 Error: {results['l2_error']['mean']:.4f}\")\n",
        "print(\"\")\n",
        "print(\"4. Colorfulness\")\n",
        "print(\"-\"*40)\n",
        "print(f\"   Predicted: {results['colorfulness']['predicted_mean']:.2f}\")\n",
        "print(f\"   Ground Truth: {results['colorfulness']['ground_truth_mean']:.2f}\")\n",
        "print(f\"   Ratio: {results['colorfulness']['ratio']:.2f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "evaluator.save_results(results, 'evaluation_results.json')\n",
        "evaluator.generate_report(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot metrics distribution\n",
        "evaluator.plot_metrics_distribution(test_loader, num_samples=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample predictions\n",
        "evaluator.visualize_samples(test_loader, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed comparison visualization\n",
        "grayscale_list = []\n",
        "predicted_list = []\n",
        "ground_truth_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for L, AB_real in test_loader:\n",
        "        L = L.to(device)\n",
        "        AB_pred = generator(L)\n",
        "        \n",
        "        for i in range(min(8, L.size(0))):\n",
        "            L_np = L[i].cpu().numpy().transpose(1, 2, 0)\n",
        "            AB_pred_np = AB_pred[i].cpu().numpy().transpose(1, 2, 0)\n",
        "            AB_real_np = AB_real[i].numpy().transpose(1, 2, 0)\n",
        "            \n",
        "            lab_pred = denormalize_lab(L_np, AB_pred_np)\n",
        "            lab_real = denormalize_lab(L_np, AB_real_np)\n",
        "            \n",
        "            rgb_pred = lab2rgb(lab_pred)\n",
        "            rgb_real = lab2rgb(lab_real)\n",
        "            \n",
        "            grayscale_list.append(L_np.squeeze())\n",
        "            predicted_list.append(rgb_pred)\n",
        "            ground_truth_list.append(rgb_real)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            psnr_list.append(calculate_psnr(AB_pred_np, AB_real_np))\n",
        "            ssim_list.append(calculate_ssim(AB_pred_np, AB_real_np))\n",
        "        \n",
        "        break  # Only one batch\n",
        "\n",
        "# Create comparison grid\n",
        "titles = [f'PSNR: {p:.1f} dB, SSIM: {s:.3f}' for p, s in zip(psnr_list, ssim_list)]\n",
        "create_comparison_grid(\n",
        "    grayscale_list[:5],\n",
        "    predicted_list[:5],\n",
        "    ground_truth_list[:5],\n",
        "    save_path='../results/evaluation/detailed_comparison.png',\n",
        "    titles=titles[:5]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze error distribution per channel\n",
        "a_errors = []\n",
        "b_errors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for L, AB_real in test_loader:\n",
        "        L = L.to(device)\n",
        "        AB_pred = generator(L)\n",
        "        \n",
        "        # Calculate per-channel errors\n",
        "        a_error = torch.abs(AB_pred[:, 0] - AB_real.to(device)[:, 0]).mean(dim=(1, 2))\n",
        "        b_error = torch.abs(AB_pred[:, 1] - AB_real.to(device)[:, 1]).mean(dim=(1, 2))\n",
        "        \n",
        "        a_errors.extend(a_error.cpu().numpy())\n",
        "        b_errors.extend(b_error.cpu().numpy())\n",
        "\n",
        "# Plot error distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('Per-Channel Error Analysis', fontsize=14, fontweight='bold')\n",
        "\n",
        "axes[0].hist(a_errors, bins=20, color='purple', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(np.mean(a_errors), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(a_errors):.3f}')\n",
        "axes[0].set_xlabel('Mean Absolute Error')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('A Channel Error (Green-Red)')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(b_errors, bins=20, color='orange', alpha=0.7, edgecolor='black')\n",
        "axes[1].axvline(np.mean(b_errors), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(b_errors):.3f}')\n",
        "axes[1].set_xlabel('Mean Absolute Error')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('B Channel Error (Blue-Yellow)')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/evaluation/channel_error_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"A Channel Mean Error: {np.mean(a_errors):.4f}\")\n",
        "print(f\"B Channel Mean Error: {np.mean(b_errors):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize error maps\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('Error Maps: Predicted vs Ground Truth', fontsize=14, fontweight='bold')\n",
        "\n",
        "with torch.no_grad():\n",
        "    L, AB_real = next(iter(test_loader))\n",
        "    L = L.to(device)\n",
        "    AB_pred = generator(L)\n",
        "    \n",
        "    for i in range(5):\n",
        "        # Calculate absolute error\n",
        "        error = torch.abs(AB_pred[i] - AB_real[i].to(device)).cpu().numpy()\n",
        "        error_map = np.mean(error, axis=0)  # Average over channels\n",
        "        \n",
        "        # Ground truth\n",
        "        L_np = L[i].cpu().numpy().transpose(1, 2, 0)\n",
        "        AB_real_np = AB_real[i].numpy().transpose(1, 2, 0)\n",
        "        lab_real = denormalize_lab(L_np, AB_real_np)\n",
        "        rgb_real = lab2rgb(lab_real)\n",
        "        \n",
        "        axes[0, i].imshow(np.clip(rgb_real, 0, 1))\n",
        "        axes[0, i].set_title('Ground Truth' if i == 0 else '')\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Error map\n",
        "        im = axes[1, i].imshow(error_map, cmap='hot', vmin=0, vmax=0.5)\n",
        "        axes[1, i].set_title('Error Map' if i == 0 else '')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "plt.colorbar(im, ax=axes[1, :].tolist(), shrink=0.6, label='Error')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/evaluation/error_maps.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "print(\"=\"*70)\n",
        "print(\"                    EVALUATION SUMMARY TABLE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<25} {'Value':<15} {'Interpretation':<30}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'PSNR (dB)':<25} {results['psnr']['mean']:.2f} ± {results['psnr']['std']:.2f}     {'Good reconstruction' if results['psnr']['mean'] > 20 else 'Needs improvement'}\")\n",
        "print(f\"{'SSIM':<25} {results['ssim']['mean']:.4f} ± {results['ssim']['std']:.4f}  {'High similarity' if results['ssim']['mean'] > 0.8 else 'Moderate similarity'}\")\n",
        "print(f\"{'L1 Error':<25} {results['l1_error']['mean']:.4f}          {'Low error' if results['l1_error']['mean'] < 0.1 else 'Moderate error'}\")\n",
        "print(f\"{'Colorfulness Ratio':<25} {results['colorfulness']['ratio']:.2f}            {'Vibrant colors' if results['colorfulness']['ratio'] > 0.8 else 'Desaturated'}\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal Test Samples: {results['num_samples']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Complete\n",
        "\n",
        "**Key Findings:**\n",
        "- PSNR and SSIM metrics indicate reconstruction quality\n",
        "- Colorfulness ratio shows how vibrant the predicted colors are\n",
        "- Error maps reveal which image regions are harder to colorize\n",
        "\n",
        "**Next Steps:**\n",
        "1. Try different loss weights (L1 lambda)\n",
        "2. Experiment with larger datasets\n",
        "3. Apply model improvements (see Phase 2 requirements)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
